{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my_func3**のテスト用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my_func.pyとの違い**\n",
    "* S/N比（SNR）を特徴量として追加 \n",
    "* 戻り値はDataFrame型 （column=音素）\n",
    "* columnに設定する際に音素の重複をなくした（それぞれに番号を割り振った）\n",
    "* 引数にnameを追加。人物ラベルの特徴量として戻り値のデータに追加する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my_func2.pyとの違い**\n",
    "* 2ch音源の入力のみ想定\n",
    "* 新たな特徴量としてpost_silence（発話が完了した音素の1つ後のPNエネルギー）を追加\n",
    "\n",
    "* S/N比（SNR）を改良：\n",
    "$$\n",
    "    SNR = \\frac{clean音声のエネルギー（PNフィルタなし音声）}{PNのエネルギー（2chの差分の音声）}\n",
    "$$\n",
    "音声のパワーとしてRMSを使用\n",
    "（参考：https://engineering.linecorp.com/ja/blog/voice-waveform-arbitrary-signal-to-noise-ratio-python/）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#音素PNエネルギーの導出\n",
    "#無音部を削除した音声からSNRを導出\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# 入力リストから重複する値のインデックス番号を取得　→　それぞれに番号を振って重複のないリストとして返す関数-------------------------------------\n",
    "def list_duplicates(input_list):\n",
    "  once = set()\n",
    "  seenOnce = once.add\n",
    "  twice = list(set( num for num in input_list if num in once or seenOnce(num) ))\n",
    "  for val in twice:\n",
    "      duplicate_num = [i for i, _x in enumerate(input_list) if _x == val]\n",
    "      for cnt,index in enumerate(duplicate_num):\n",
    "          input_list[index] = input_list[index]+\"_{}\".format(cnt+1)\n",
    "\n",
    "  return input_list\n",
    "\n",
    "\n",
    "# 1ch音声用：入力音声配列のSN比（dB）を導出する関数-------------------------------------\n",
    "def snr_1ch(a, axis=0, ddof=0): #ddof : Degrees of freedom correction for standard deviation. Default is 0.\n",
    "    a = np.asanyarray(a)\n",
    "    m = a.mean(axis)\n",
    "    sd = a.std(axis=axis, ddof=ddof)\n",
    "    return 20*np.log10(abs(np.where(sd == 0, 0, m/sd))) #np.where：sd==0なら0,それ以外ならm/sdを返す（m/sdが発散するのを防ぐ）\n",
    "\n",
    "# 2ch音声用：入力音声配列のSN比（dB）を導出する関数-------------------------------------\n",
    "def snr_2ch(wav):\n",
    "    # wav_clean = wav[:,0] #PNなし音声の抽出\n",
    "    # signal = np.sqrt((wav_clean ** 2).sum() / wav_clean.size)\n",
    "    # wav_diff = wav[:,1]- wav[:,0]   #2ch音源(clean,PN)で、差分を取る\n",
    "    # noise = np.sqrt((wav_diff ** 2).sum() / wav_diff.size)\n",
    "    # return 20*np.log10(np.where(noise==0, 0, signal/noise)) #np.where：noise==0なら0,それ以外ならSNRを返す（発散するのを防ぐ）\n",
    "    wav_clean = wav[:,0] #PNなし音声の抽出\n",
    "    signal = np.sqrt((wav_clean ** 2).sum() / wav_clean.size)\n",
    "    wav_noise = wav[:,1]  #PNあり音声\n",
    "    wav_noise_only = wav[:,1] - wav[:,0] \n",
    "    noise = np.sqrt((wav_noise_only ** 2).sum() / wav_noise_only.size)\n",
    "    return 20*np.log10(np.where(noise==0, 0, signal/noise)) #np.where：noise==0なら0,それ以外ならSNRを返す（発散するのを防ぐ）\n",
    "\n",
    "\n",
    "\n",
    "#音素ごとのPNエネルギーを導出する関数-----------------------------------\n",
    "def pnd(name,input_path):\n",
    "    wav_data, fs = sf.read(input_path)\n",
    "    # wav_data, fs = sf.read('wav/{}.wav'.format(filename))\n",
    "\n",
    "    file_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "\n",
    "    #マイクが2chの時のみこのスクリプトを実行する\n",
    "    if len(wav_data.shape)==2:\n",
    "        # wav_data = wav_data[:,0]    #PNなし音声\n",
    "        wav_data = wav_data[:,1]    #PNあり音声\n",
    "        # wav_data = wav_data[:,1]- wav_data[:,0]   #2ch音源(clean,PN)で、差分を取る場合\n",
    "\n",
    "\n",
    "    #STFT\n",
    "    hamm_window = signal.windows.hamming(4096,sym=True) #sym=Trueでperiodicになる（スペクトル解析用）\n",
    "    f,t,Sxx = signal.spectrogram(wav_data, fs=fs, window=hamm_window ,noverlap=2048)\n",
    "    t_dur = (t[0]+t[1])/2 #１フレームの時間\n",
    "    \n",
    "    # #図の描画\n",
    "    # plt.pcolormesh(t, f, 10*np.log(Sxx)) #intensityを修正\n",
    "    # plt.ylabel('Frequency [Hz]')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.ylim([f[1], 500])\n",
    "    # cbar = plt.colorbar() #カラーバー表示のため追加\n",
    "    # cbar.ax.set_ylabel(\"Intensity [dB]\") #カラーバーの名称表示のため追加\n",
    "    # plt.show()\n",
    "\n",
    "    # #パターン１：低周波域のエネルギーをそのまま集計(0~40Hz)\n",
    "    # Sxx2 = 10*np.log(Sxx) #スペクトルのエネルギーをdB値に変換\n",
    "    # low_ene = np.zeros(len(t))\n",
    "    # low_ene = Sxx2[0:4,:].sum(axis=0) #0~43Hzのパワースペクトルを集計\n",
    "    # plt.plot(t,low_ene)\n",
    "    # plt.ylabel('Accumulated Intensity [dB] (0~43Hz)')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "\n",
    "    #パターン２：低周波域のエネルギーを集計して標準化を行う(0~40Hz)\n",
    "    Sxx2 = 10*np.log(Sxx) #スペクトルのエネルギーをdB値に変換\n",
    "    low_ene = np.zeros(len(t))\n",
    "    low_ene = Sxx2[0:4,:].sum(axis=0) #0~43Hzのパワースペクトルを集計\n",
    "    low_ene_norm = scipy.stats.zscore(low_ene)\n",
    "    # plt.scatter(t,low_ene_norm)\n",
    "    # plt.plot(t,low_ene_norm)\n",
    "    # plt.ylabel('Accumulated Intensity [dB] (0~43Hz)')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.show()\n",
    "\n",
    "    #Juliusの音素データを読み込み（sample.lab）\n",
    "    with open('wav/{}.lab'.format(file_name)) as f:\n",
    "        lines = f.readlines() #readlinesで一括読み込み\n",
    "    f.close\n",
    "    cnt = 0\n",
    "    for line in lines:\n",
    "        lines[cnt] = line.split()\n",
    "        cnt += 1\n",
    "    pho_seg = pd.DataFrame(lines)\n",
    "    pho_seg = pho_seg.drop(0) #無音部（最初）を消去する\n",
    "    pho_seg = pho_seg.drop(len(pho_seg)) #無音部（最後）を消去する\n",
    "    # pho_seg = pho_seg.drop(0, axis=0) #無音部（最初）を消去する\n",
    "    # pho_seg = pho_seg.drop(len(pho_seg),axis=0) #無音部（最後）を消去する\n",
    "    pho_seg[0] = pho_seg[0].astype(float)\n",
    "    pho_seg[1] = pho_seg[1].astype(float)\n",
    "\n",
    "\n",
    "    # #おまけ：音素データとPNデータの統合結果をグラフにプロット\n",
    "    # ymin, ymax = -2.5,3\n",
    "    # plt.figure(figsize=(12,4)) #図のサイズのデフォルトは(6.4, 4.8)\n",
    "    # plt.scatter(t,low_ene_norm)\n",
    "    # plt.plot(t,low_ene_norm)\n",
    "    # plt.ylabel('Accumulated Intensity [dB] (0~43Hz)')\n",
    "    # plt.xlabel('Time [sec]')\n",
    "    # plt.ylim(ymin,ymax)\n",
    "    # plt.xlim(pho_seg.iat[0,0]-1.5*t_dur, pho_seg.iat[-1,1]+1.5*t_dur)\n",
    "    # for i in range(len(pho_seg)):\n",
    "    #     plt.vlines(pho_seg.iat[i,0],ymin,ymax,colors='blue', linestyle='dashed', linewidth=1)\n",
    "    #     plt.text((pho_seg.iat[i,0] + pho_seg.iat[i,1])/2, ymax-0.3, pho_seg.iat[i,2], size=10,horizontalalignment=\"center\", color='red')\n",
    "    # plt.vlines(pho_seg.iat[i,1],ymin,ymax,colors='blue', linestyle='dashed', linewidth=1)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    #音素データとPNデータの統合　▶　音素ごとのPNエネルギーの導出\n",
    "    pho_PN = np.zeros(len(pho_seg)) #音素ごとの平均PNエネルギー\n",
    "    pre_silence_PN = 0 #無音部のPNエネルギー（音素が始まる1つ前のPNエネルギーのみ参照）\n",
    "    for i in range(len(pho_seg)):\n",
    "        pho_sta = pho_seg.iat[i,0]\n",
    "        pho_end = pho_seg.iat[i,1]  \n",
    "        cnt = 0\n",
    "        temp = 0\n",
    "        pre_num = -1\n",
    "        post_num = 0\n",
    "        for j in range(len(t)): \n",
    "            if pho_sta < t[j] :\n",
    "                if pre_num == -1: #音素範囲外の１つ前のフレーム番号を保存\n",
    "                    pre_num = j-1\n",
    "                if t[j] < pho_end: #音素フレーム間（startとendの間）にPNフレームがある時\n",
    "                    cnt += 1\n",
    "                    temp += low_ene_norm[j]\n",
    "                else:\n",
    "                    if post_num == 0: #音素範囲外の１つ外のフレーム番号を保存\n",
    "                        post_num = j  \n",
    "    \n",
    "        if temp > 0: #音素のstartとendの間にPNフレームが無いとき\n",
    "            pho_PN[i] = temp/cnt\n",
    "        else: #音素のstartとendの間にPNフレームが無いとき\n",
    "            pho_PN[i] = (low_ene_norm[pre_num] + low_ene_norm[post_num])/2\n",
    "        \n",
    "        if i == 0 : #1つ目の音素が検出した際、音素が始まる1つ前のPNエネルギーを無音部のPNエネルギーとして取得\n",
    "            pre_silence_PN = low_ene_norm[pre_num]\n",
    "    \n",
    "    #無音部のPNエネルギー（発話が完了した音素の1つ後のPNエネルギーのみ参照）\n",
    "    post_silence_PN = low_ene_norm[post_num] \n",
    "\n",
    "\n",
    "    seg_columns = pho_seg[2].values.tolist() #Series to list\n",
    "    seg_columns = list_duplicates(seg_columns) #音素の重複をなくす（重複しているものに順番をつける）\n",
    "    \n",
    "    pho_PN = [pho_PN] #DataFrame用に２次元に変更\n",
    "    pho_PN = pd.DataFrame(pho_PN,columns=seg_columns)\n",
    "\n",
    "\n",
    "    #無音部のPNエネルギーを特徴量として追加\n",
    "    pho_PN.insert(loc=0, column='pre_silence', value=pre_silence_PN)\n",
    "    pho_PN['post_silence'] = post_silence_PN\n",
    "\n",
    "\n",
    "    #無音部を消去した音声からSNRを導出する--------------------------------------------\n",
    "    speech_sta = pho_seg.iat[0,0] #発声し始めのタイミング\n",
    "    speech_end = pho_seg.iat[-1,1] #発声し終わりのタイミング\n",
    "\n",
    "    # 元ファイル名に_cutをつけてリネーム\n",
    "    # root, ext = os.path.splitext(input_path)\n",
    "    # out_path = os.path.join(root + '_cut' + '.wav')\n",
    "    out_path = os.path.join('cut_wav/temp_cut.wav')\n",
    "\n",
    "    # wavファイルの読み込み\n",
    "    sound = AudioSegment.from_wav(input_path)\n",
    "\n",
    "    # 音声区間のみを抽出\n",
    "    sound1 = sound[speech_sta*1000:speech_end*1000]\n",
    "\n",
    "    # リネームして一旦出力\n",
    "    sound1.export(out_path, format=\"wav\")\n",
    "\n",
    "    #soundfileで再読み込み\n",
    "    # wav_data, fs = sf.read('../RealTimeVerification/input_wav/input.wav')\n",
    "    wav_data, fs = sf.read(out_path)\n",
    "\n",
    "    #SNRの導出し、pho_PNene配列に追加\n",
    "    if len(wav_data.shape)==2: #入力音声が2chによる時\n",
    "        snr = snr_2ch(wav_data)\n",
    "    else: \n",
    "        snr = snr_1ch(wav_data)\n",
    "    pho_PN.insert(loc=0, column='SNR', value=snr)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    #人物ラベルを追加\n",
    "    pho_PN.insert(loc=0, column='name', value=name)\n",
    "\n",
    "    return pho_PN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pnd('takemae','backup/wav_backup/sample.wav'))\n",
    "# print(pnd('takemae','backup/input_wav_backup/takemae1f.wav'))\n",
    "# print(pnd('kakei','C:/Users/kktak/OneDrive - Osaka University/音源/f/prompt_f_sonsitsu/kakei/20211028/kakei10f.wav'))\n",
    "# print(pnd('azuma',r'C:/Users/kktak/OneDrive - Osaka University/音源/f/prompt_f_sonsitsu/azuma/20220617/220617_004-01.wav'))\n",
    "# print(pnd('takemae','C:/Users/kktak/OneDrive - Osaka University/音源/temp_f/takemae/20220518/220518_002.WAV'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9be4ecf491ae9f70b685324f1f1f604ffbefa02b1383b83a7c51bb175c0f0a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
